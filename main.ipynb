{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57326bf",
   "metadata": {},
   "source": [
    "<h2>Importacion de bibliotecas</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6631350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, mean_absolute_error\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import joblib\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd2e38e",
   "metadata": {},
   "source": [
    "<h2>Se cargan y se limpia el dataset de clientes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "025b8add",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cargar_y_limpiar_clientes(path):\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"Original size (Clients):\", len(df))\n",
    "\n",
    "    df['genero'] = df['genero'].replace(' ', np.nan)\n",
    "    invalid_genres = df[df['genero'].isna()]['id'].tolist()\n",
    "    df = df.dropna(subset=['genero'])\n",
    "\n",
    "    df['fecha_nacimiento'] = pd.to_datetime(df['fecha_nacimiento'], errors='coerce')\n",
    "    invalid_births = df[df['fecha_nacimiento'] < pd.Timestamp('1900-01-01')]['id'].tolist()\n",
    "    df = df[df['fecha_nacimiento'] >= pd.Timestamp('1900-01-01')]\n",
    "\n",
    "    del_ids = set(invalid_genres + invalid_births)\n",
    "    print(\"Final size (Clients):\", len(df))\n",
    "    print(\"IDs eliminados (Clients):\", del_ids)\n",
    "    return df, del_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bded8e",
   "metadata": {},
   "source": [
    "<h2>Se carga y se limpia el dataset de clientes</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2725e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cargar_y_limpiar_transacciones(path, del_ids):\n",
    "    df = pd.read_csv(path)\n",
    "    print(\"Original size (Transactions):\", len(df))\n",
    "\n",
    "    df = df[~df['id'].isin(del_ids)]\n",
    "    df['giro_comercio'] = df['giro_comercio'].replace('4121', 'SERVICIOS DE TRANSPORTE POR AUTOM√ìVIL')\n",
    "    df = df.dropna()\n",
    "\n",
    "    replace_map = {\n",
    "        '7ELEVEN': '7 ELEVEN', 'TOTAL PLAY': 'TOTALPLAY',\n",
    "        'APLAZ': 'APLAZO', 'DIDIFOOD': 'DIDI FOOD',\n",
    "        'MERCADOPAGO': 'MERCADO PAGO', 'SMARTFIT': 'SMART FIT',\n",
    "        'WAL-MART': 'WALMART', 'MI ATT': 'AT&T', 'ATT': 'AT&T'\n",
    "    }\n",
    "    df['comercio'] = df['comercio'].replace(replace_map)\n",
    "    print(\"Cleaned size (Transactions):\", len(df))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3fcb12",
   "metadata": {},
   "source": [
    "<h2>Construir un dataset combinando el dataset de clientes y transacciones</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5312d8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def construir_dataset(merged_df):\n",
    "    merged_df['fecha'] = pd.to_datetime(merged_df['fecha'], errors='coerce')\n",
    "    freq = merged_df.groupby(['id', 'comercio']).size().reset_index(name='n_tx')\n",
    "    rec_pairs = freq[freq['n_tx'] >= 3][['id','comercio']]\n",
    "    tx_rec = merged_df.merge(rec_pairs, on=['id','comercio'], how='inner')\n",
    "\n",
    "    rows = []\n",
    "    for (cli, com), grp in tx_rec.groupby(['id','comercio']):\n",
    "        grp = grp.sort_values('fecha').reset_index(drop=True)\n",
    "        grp['days_diff'] = grp['fecha'].diff().dt.days\n",
    "        for i in range(2, len(grp) - 1):\n",
    "            curr = grp.loc[i]\n",
    "            prev = grp.loc[:i-1]\n",
    "            nxt  = grp.loc[i+1]\n",
    "            delta = (nxt['fecha'] - curr['fecha']).days\n",
    "            rows.append({\n",
    "                'd√≠as_desde_√∫ltima': curr['days_diff'],\n",
    "                'monto_actual': curr['monto'],\n",
    "                'promedio_montos_previos': prev['monto'].mean(),\n",
    "                'periodicidad_media': prev['days_diff'].mean(),\n",
    "                'trans_previas': i,\n",
    "                'tuvo_pr√≥xima_en_30d': int(delta <= 30),\n",
    "                'd√≠as_hasta_pr√≥xima': delta,\n",
    "                'monto_pr√≥ximo': nxt['monto']\n",
    "            })\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # üßº Filtrar valores extremos\n",
    "    df = df[\n",
    "        (df['d√≠as_hasta_pr√≥xima'] > 0) &\n",
    "        (df['d√≠as_hasta_pr√≥xima'] <= 60) &\n",
    "        (df['monto_pr√≥ximo'] > 0) &\n",
    "        (df['monto_pr√≥ximo'] <= 10000)\n",
    "    ]\n",
    "\n",
    "    print(\"Training examples:\", df.shape)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c44c36a",
   "metadata": {},
   "source": [
    "<h2>Se entrenan, se guardan los modelos en un archivo y se hace pruebas en las metricas </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e60fab2",
   "metadata": {},
   "source": [
    "Se hacen un division de los datos, donde se van a usar 3 modelos:\n",
    "\n",
    "* Se uso XGBClassifier para encontrar la probabilidad que un cliente haga una compra en los siguientes 30 dias\n",
    "* Se uso XGBRegressor para predecir la proxima fecha a realizar una compra\n",
    "* Se uso XGBRegressor para estimar el monto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e192dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entrenar_y_guardar_modelos(model_df):\n",
    "    features = ['d√≠as_desde_√∫ltima', 'monto_actual', 'promedio_montos_previos', 'periodicidad_media', 'trans_previas']\n",
    "    X = model_df[features]\n",
    "    y = model_df['tuvo_pr√≥xima_en_30d']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    clf = XGBClassifier(n_estimators=100, max_depth=4, random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_proba = clf.predict_proba(X_test)[:,1]\n",
    "    print(\"ROC-AUC Clasificador:\", roc_auc_score(y_test, y_proba))\n",
    "\n",
    "    # Regresi√≥n solo para positivos\n",
    "    pos_mask = y_train == 1\n",
    "    X_reg = X_train[pos_mask]\n",
    "    y_days = model_df.loc[X_reg.index, 'd√≠as_hasta_pr√≥xima']\n",
    "    y_amt  = model_df.loc[X_reg.index, 'monto_pr√≥ximo']\n",
    "\n",
    "    # ‚ûï Transformaciones logar√≠tmicas\n",
    "    y_days_log = np.log1p(y_days)\n",
    "    y_amt_log = np.log1p(y_amt)\n",
    "\n",
    "    reg_days = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        random_state=42\n",
    "    )\n",
    "    reg_days.fit(X_reg, y_days_log)\n",
    "\n",
    "    reg_amt = XGBRegressor(\n",
    "        n_estimators=200,\n",
    "        max_depth=4,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    reg_amt.fit(X_reg, y_amt_log)\n",
    "\n",
    "    # Evaluaci√≥n\n",
    "    test_mask = y_proba >= 0.5\n",
    "    X_reg_test = X_test[test_mask]\n",
    "    y_days_true = model_df.loc[X_reg_test.index, 'd√≠as_hasta_pr√≥xima']\n",
    "    y_amt_true  = model_df.loc[X_reg_test.index, 'monto_pr√≥ximo']\n",
    "\n",
    "    # ‚ûï Invertimos la transformaci√≥n logar√≠tmica\n",
    "    y_days_pred = np.expm1(reg_days.predict(X_reg_test))\n",
    "    y_amt_pred  = np.expm1(reg_amt.predict(X_reg_test))\n",
    "\n",
    "    print(\"MAE d√≠as:\", mean_absolute_error(y_days_true, y_days_pred))\n",
    "    print(\"MAE monto:\", mean_absolute_error(y_amt_true, y_amt_pred))\n",
    "\n",
    "    joblib.dump(clf, 'model_clf.xgb')\n",
    "    joblib.dump(reg_days, 'model_reg_days.xgb')\n",
    "    joblib.dump(reg_amt, 'model_reg_amt.xgb')\n",
    "    print(\"Modelos guardados exitosamente.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a84116",
   "metadata": {},
   "source": [
    "<h2>Se inicializa el programa</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b26faa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size (Clients): 1000\n",
      "Final size (Clients): 994\n",
      "IDs eliminados (Clients): {'8e0ace2e33ad0399023ba42beb7b0fe1b10ff04d', 'b354f9d82e4ea716310fb3daace1559c6cc9390e', '9c6ef874db857c5669e12c72c5d9d6d24d1eef62', '50f4ad84e793535b162be0c9d2bd7f23b154085a', '9980f12e32711330d5f58460e169e6207afda041', 'e6e3eb2708ad54876105584985dad63d24c38190'}\n",
      "Original size (Transactions): 346011\n",
      "Cleaned size (Transactions): 337998\n",
      "Merged size: 337998\n",
      "Training examples: (208987, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/.local/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [09:54:40] WARNING: /workspace/src/learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Clasificador: 0.8920235104074576\n",
      "MAE d√≠as: 6.110006332397461\n",
      "MAE monto: 22.240518950349713\n",
      "Modelos guardados exitosamente.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    try:\n",
    "        df_clients, del_ids = cargar_y_limpiar_clientes('db_clients.csv')\n",
    "        df_transactions = cargar_y_limpiar_transacciones('db_transactions.csv', del_ids)\n",
    "        merged_df = df_transactions.merge(df_clients, on='id', how='inner')\n",
    "        print(\"Merged size:\", len(merged_df))\n",
    "\n",
    "        model_df = construir_dataset(merged_df)\n",
    "        if model_df.empty:\n",
    "            print(\"No hay suficientes datos para entrenar.\")\n",
    "            sys.exit(0)\n",
    "\n",
    "        entrenar_y_guardar_modelos(model_df)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Ocurri√≥ un error:\", str(e))\n",
    "        sys.exit(1)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
